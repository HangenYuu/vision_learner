{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjX5gh7vmB90"
      },
      "source": [
        "## Front Matter\n",
        "I want the Notebook to be as informative as possible, but model creating and training process follows some standard procedure that I do not want to repeat. Therefore, if you can, spend time reading the `PROLOGUE/Routine.ipynb` Notebook first."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G7BZ56Y3mG2o"
      },
      "source": [
        "# Paper Implementation - VGG16\n",
        "Hello, this is my first milestone project - implementation of the VGG16 architecture from the paper [\"Very Deep Convolutional Networks for Large-Scale Image Recognition\"](https://arxiv.org/abs/1409.1556). The paper explored the effect of increasing layers on a model based on the filter size of $3*3$ that we previously explored in the `TinyCNN` notebook (that is also the reason why that architecture is called TinyVGG). The architecture was the runner-up in the ImageNet 2014 Challenge for classification.\n",
        "\n",
        "16 in VGG16 stands for 16 layers, where they are based on two basic units: convolution with filter size $3*3$, stride $1$, padding $1$ and max-pooling with window size $2*2$, stride $2$. The table shown below, taken from the paper abovem, is the architecture for each of the VGG configuration. In this notebook, we will implement the VGG16-D one.\n",
        "\n",
        "![](Screenshot 2022-12-23 at 14-16-30 () - 10.48550_arxiv.1409.1556.pdf.png)\n",
        "\n",
        "In this first notebook, we will focus on getting and transforming the data first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHEZ-gjM3S3N"
      },
      "source": [
        "## Downloading and extracting data\n",
        "The task for our model will be classification, using a bigger dataset called [Food101](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html). This is a built-in PyTorch dataset, so the processing can be fairly straightforward. However, it is not fun, so let's take the [Kaggle version](https://www.kaggle.com/datasets/kmader/food41?select=food_c101_n1000_r384x384x3.h5) and process it to what we want.\n",
        "\n",
        "First, downloading data from Kaggle. The easy way: you can download the zip file (~6 GB), upload it to Google Drive, and then mount Google Drive to Colab. . The slightly harder: you will need to sign up and obtain a Kaggle token, and then use the `kaggle` module to download the data. Let's do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0eajahkkT9Y",
        "outputId": "62d2c00f-dedc-4f20-aa54-4dd65057b70b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0+cu116)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoGcGDeU5oZc",
        "outputId": "a74a7b20-8c22-49d9-ac28-8fd74719e9f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.8/dist-packages (0.14.0)\n",
            "Collecting mlxtend\n",
            "  Downloading mlxtend-0.21.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from mlxtend) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from mlxtend) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.8/dist-packages (from mlxtend) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from mlxtend) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.8/dist-packages (from mlxtend) (1.3.5)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.8/dist-packages (from mlxtend) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from mlxtend) (57.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.24.2->mlxtend) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->mlxtend) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.2->mlxtend) (3.1.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Installing collected packages: mlxtend\n",
            "  Attempting uninstall: mlxtend\n",
            "    Found existing installation: mlxtend 0.14.0\n",
            "    Uninstalling mlxtend-0.14.0:\n",
            "      Successfully uninstalled mlxtend-0.14.0\n",
            "Successfully installed mlxtend-0.21.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade mlxtend kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ0Vz4p153pc"
      },
      "source": [
        "First, we need a variable to keep track of the environment we are in as it is different to run the notebook right on Kaggle and run it anywhere else (from the teaching of a Kaggle Grandmaster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqFHdTF85vdF"
      },
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Keep an environment variable\n",
        "iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcrFy3Lh661F"
      },
      "source": [
        "Next, based on the [docs](https://github.com/Kaggle/kaggle-api), we will need to create a `/.kaggle/kaggle.json`. You can go to File Explorer and create a folder in your machine, or we can code that. I will code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F11gkoel6oas"
      },
      "outputs": [],
      "source": [
        "# Paste your API here (I have run and then deleted mine)\n",
        "creds = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bmOkZ8763En"
      },
      "outputs": [],
      "source": [
        "cred_path = Path('~/.kaggle/kaggle.json').expanduser()\n",
        "if not cred_path.exists():\n",
        "    cred_path.parent.mkdir(exist_ok=True)\n",
        "    cred_path.write_text(creds)\n",
        "    cred_path.chmod(0o600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkaWDwHQ84_9"
      },
      "source": [
        "Next, let's use the method `dataset_download_cli` to download and unzip data files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGMzBXkk7he-",
        "outputId": "7b2bbd7c-d765-43eb-bc57-09f497088b6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ref                                                             title                                             size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "meirnizri/covid19-dataset                                       COVID-19 Dataset                                   5MB  2022-11-13 15:47:17          11624        344  1.0              \n",
            "michals22/coffee-dataset                                        Coffee dataset                                    24KB  2022-12-15 20:02:12           2316         63  1.0              \n",
            "thedevastator/jobs-dataset-from-glassdoor                       Salary Prediction                                  3MB  2022-11-16 13:52:31           7233        155  1.0              \n",
            "thedevastator/unlock-profits-with-e-commerce-sales-data         E-Commerce Sales Dataset                           6MB  2022-12-03 09:27:17           1694         48  1.0              \n",
            "ahmettalhabektas/argentina-car-prices                           Argentina car prices                               8KB  2022-12-05 09:05:21            745         34  1.0              \n",
            "mvieira101/global-cost-of-living                                Global Cost of Living                              1MB  2022-12-03 16:37:53           3260         69  0.9705882        \n",
            "thedevastator/uncovering-wage-disparities-in-pennsylvania-s-hi  Higher Education Wages                           223KB  2022-12-04 15:42:36           1172         36  1.0              \n",
            "danela/fatal-alligator-attacks-us                               Fatal Alligator Attacks US                         5KB  2022-12-15 16:37:57            350         26  1.0              \n",
            "kabhishm/best-selling-music-artists-of-all-time                 Best Selling Music Artists of All Time             3KB  2022-12-09 07:04:29            920         39  0.9411765        \n",
            "swaptr/fifa-world-cup-2022-statistics                           FIFA World Cup 2022 Team Data                     15KB  2022-12-19 00:29:15           2478         58  0.9705882        \n",
            "whenamancodes/predict-diabities                                 Predict Diabetes                                   9KB  2022-11-09 12:18:49           7549        119  1.0              \n",
            "die9origephit/fifa-world-cup-2022-complete-dataset              Fifa World Cup 2022: Complete Dataset              7KB  2022-12-18 22:51:11           1991         83  0.9411765        \n",
            "mattop/alcohol-consumption-per-capita-2016                      Alcohol Consumption Per Capita 2016                4KB  2022-12-09 00:03:11           1186         43  1.0              \n",
            "swaptr/fifa-world-cup-2022-match-data                           FIFA World Cup 2022 Match Data                     7KB  2022-12-19 00:30:28           1201         33  1.0              \n",
            "laibaanwer/superstore-sales-dataset                             SuperStore Sales Dataset                           2MB  2022-12-07 08:53:32           1339         36  1.0              \n",
            "thedevastator/discovering-hidden-trends-in-global-video-games   Discovering Hidden Trends in Global Video Games   56KB  2022-12-03 11:21:47            727         39  1.0              \n",
            "thedevastator/the-ultimate-netflix-tv-shows-and-movies-dataset  Netflix TV Shows and Movies (2022 Updated)         2MB  2022-11-27 20:41:41           2272         39  1.0              \n",
            "catherinerasgaitis/mxmh-survey-results                          Music & Mental Health Survey Results              22KB  2022-11-21 10:03:12           2991         68  1.0              \n",
            "kabhishm/imdb-100-movie-titles                                  IMDB 100 Movies                                    9KB  2022-12-07 11:36:06            655         32  0.9411765        \n",
            "tirendazacademy/fifa-world-cup-2022-tweets                      FIFA World Cup 2022 Tweets                         1MB  2022-12-08 19:43:37            958         34  1.0              \n"
          ]
        }
      ],
      "source": [
        "# Sanity check\n",
        "!kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlqqxH249lnd"
      },
      "outputs": [],
      "source": [
        "path = Path('kmader/food41')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obGTr0Sl9jed",
        "outputId": "50dd4a2a-344a-45dc-fd1f-709b016716b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading food41.zip to /content\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5.30G/5.30G [02:58<00:00, 31.9MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "if not iskaggle and not path.exists():\n",
        "    import kaggle\n",
        "    kaggle.api.dataset_download_cli(str(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cjuqNb7-y82"
      },
      "source": [
        "We have the data in the zip file. Now all we need to do is to extract them out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPymJH-p_HqG"
      },
      "outputs": [],
      "source": [
        "folder_path = Path('food41')\n",
        "os.mkdir(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAzPGZSC-7Nv"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zipfile.ZipFile(f'{folder_path}.zip').extractall(folder_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXrQ72h9_7cX"
      },
      "source": [
        "## Food-101\n",
        "The dataset is introduced in this [paper](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/ttps://), consisting of 101 classes, each with 750 training and 250 testing examples, totalling 1000 images each. The dataset comes with a metadata folder, giving information about which image should go into which subset, which is great! The data was already split into traing and testing examples, but we also need a *validation set*. The testing examples were manually selected to contain noise and challenge the model, so we will not touch that, but we will split the training set further to create a validation set. Now, creating a good validation set is [an art](https://www.fast.ai/posts/2017-11-13-validation-sets.html), but here we will jsut use good ol' random splitting.\n",
        "\n",
        "First, we will need to format the in the `images` folder into `train` and `test` folders. Next, we will load the data. The process is quite the same, what's new this time is we will random split the training data into training set and validation set, as well as applying more transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVpUhgwGB6lI"
      },
      "outputs": [],
      "source": [
        "# Generic torch process\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Specifically for computer vision\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Other module(s)\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import json\n",
        "import shutil\n",
        "import itertools "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt9pTx7iCZpI",
        "outputId": "47e1c984-b949-4fd0-dac3-0af10da52e68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "750 ['apple_pie/960233', 'apple_pie/960669', 'apple_pie/962315', 'apple_pie/966595', 'apple_pie/973088', 'apple_pie/973428', 'apple_pie/98352', 'apple_pie/98449', 'apple_pie/987860', 'apple_pie/997124']\n",
            "250 ['apple_pie/885848', 'apple_pie/886793', 'apple_pie/904832', 'apple_pie/908367', 'apple_pie/963140', 'apple_pie/981895', 'apple_pie/984571', 'apple_pie/986844', 'apple_pie/99556', 'apple_pie/997950']\n"
          ]
        }
      ],
      "source": [
        "with open('/content/food41/meta/meta/train.json', 'r') as fp:\n",
        "    train_dict = json.load(fp)\n",
        "with open('/content/food41/meta/meta/test.json', 'r') as fp:\n",
        "    test_dict = json.load(fp)\n",
        "print(len(train_dict['apple_pie']), train_dict['apple_pie'][-10:])\n",
        "print(len(test_dict['apple_pie']), test_dict['apple_pie'][-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWF_jS8kTtck"
      },
      "source": [
        "The list value of a dictionary key contains the strings that are the file paths of the images without the extension. We will use this to copy the images to proper folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLuvUHFjJgDJ"
      },
      "outputs": [],
      "source": [
        "os.mkdir('data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwwjcxX5MBgJ"
      },
      "outputs": [],
      "source": [
        "new_data_path = Path('data')\n",
        "original_data_path = Path('food41/images')\n",
        "new_folders = ['train', 'test']\n",
        "for folder in new_folders:\n",
        "    if folder == 'train':\n",
        "        for key, value in train_dict.items():\n",
        "            value_set = set(value)\n",
        "            if not os.path.exists(new_data_path/folder/key):\n",
        "                os.mkdir(new_data_path/folder/key)\n",
        "            for image in os.listdir(original_data_path/key):\n",
        "                image_path = key + '/' + image\n",
        "                image_path = image_path.split('.')[0]\n",
        "                if image_path in value_set:\n",
        "                    shutil.copy(original_data_path/key/image, new_data_path/folder/key/image)\n",
        "    else:\n",
        "        for key, value in test_dict.items():\n",
        "            value_set = set(value)\n",
        "            if not os.path.exists(new_data_path/folder/key):\n",
        "                os.mkdir(new_data_path/folder/key)\n",
        "            for image in os.listdir(original_data_path/key):\n",
        "                image_path = key + '/' + image\n",
        "                image_path = image_path.split('.')[0]\n",
        "                if image_path in value_set:\n",
        "                    shutil.copy(original_data_path/key/image, new_data_path/folder/key/image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrO1s8QEXYv_"
      },
      "source": [
        "And we are done! Now we can load data as we like!\n",
        "\n",
        "But first, let's write some transformations for the images to perform data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXbEbKmZYAU0"
      },
      "source": [
        "## Data Augmentation\n",
        "This is a technique to generate more training data by performing operations on the original data (such as flipping, shearing, rotating for images). The artificial data should generate the same output as the original one, but they are different, so hopefully the model is encouraged to learn the general pattern of the data instead of overfitting. Data augmentation is usually seen in training, but there is a technique called \"test-time augmentation\" that has been passed down among the Kaggle Grandmaster and implemented in the library [fastai](https://docs.fast.ai/learner.html#tta).\n",
        "\n",
        "For the transformations, first we have the staples: `ToTensor()`, which turns images to `torch.tensor` objects. You may notice the `Normalize()` with some arbitrary parameters (the first is a list of means for each color channel and the second is a list of standard deviations for each color channel). These are parameters for the normalization of [ImageNet](https://image-net.org/index.php) dataset and are required by all PyTorch pre-trained models. This may not necessarily be true for our data, but it can be used. PyTorch also recommends having images of size $224*224$ pixels, so we use resize to that. The other transformations do what it is called for, with parameters for angle, probability, etc. (Explore more transformations on PyTorch [docs](https://pytorch.org/vision/stable/transforms.html).) Finally, we call Compose to stack these transformations together.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTn8MTCnMfgC"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                                      transforms.RandomRotation(35),\n",
        "                                      transforms.RandomVerticalFlip(0.27),\n",
        "                                      transforms.RandomHorizontalFlip(0.27),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "valid_n_test_transforms = transforms.Compose([transforms.Resize(224),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gLYF2bpcqp4"
      },
      "outputs": [],
      "source": [
        "data_dir = Path('data')\n",
        "train_dir = data_dir/'train'\n",
        "test_dir = data_dir/'test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47DFtYLuc21v"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.ImageFolder(train_dir, transform = train_transforms)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "2043299c89c8cd0b4d1a6f5cf4529bd58e6a4e0fe3181a25e0d328c821cdc5c5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
